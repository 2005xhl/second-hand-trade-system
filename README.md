# å•†å“ä¿¡æ¯çˆ¬è™«ä½¿ç”¨è¯´æ˜

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

è¿™ä¸ªçˆ¬è™«ç³»ç»Ÿå¯ä»¥ï¼š
1. **çˆ¬å–å•†å“ä¿¡æ¯**ï¼šä»äºŒæ‰‹äº¤æ˜“å¹³å°è·å–å•†å“æ•°æ®
2. **ä¸‹è½½å›¾ç‰‡**ï¼šè‡ªåŠ¨ä¸‹è½½å•†å“å›¾ç‰‡åˆ°æœ¬åœ°
3. **å­˜å‚¨æ•°æ®**ï¼šå°†å•†å“ä¿¡æ¯ä¿å­˜åˆ°MySQLæ•°æ®åº“
4. **æ•°æ®æ¸…æ´—**ï¼šè‡ªåŠ¨æ¸…ç†å’Œæ ¼å¼åŒ–æ•°æ®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. é…ç½®æ•°æ®åº“

ç¼–è¾‘ `spider.py` ä¸­çš„æ•°æ®åº“é…ç½®ï¼š

```python
db_config = {
    'host': '127.0.0.1',
    'port': 3306,
    'user': 'root',
    'password': 'ä½ çš„æ•°æ®åº“å¯†ç ',  # ä¿®æ”¹è¿™é‡Œ
    'database': 'used_goods_platform',
    'charset': 'utf8mb4'
}
```

### 3. è¿è¡Œçˆ¬è™«

```bash
python spider.py
```

ç„¶åé€‰æ‹©ï¼š
- **é€‰é¡¹1**ï¼šæ¨¡æ‹Ÿçˆ¬è™«ï¼ˆç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œæ¨èå…ˆç”¨è¿™ä¸ªæµ‹è¯•ï¼‰
- **é€‰é¡¹2**ï¼šçœŸå®çˆ¬è™«ï¼ˆéœ€è¦æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´ä»£ç ï¼‰

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨æ¨¡æ‹Ÿçˆ¬è™«ï¼ˆæ¨èï¼‰

```python
from spider import MockSpider

db_config = {
    'host': '127.0.0.1',
    'port': 3306,
    'user': 'root',
    'password': '123456',
    'database': 'used_goods_platform',
    'charset': 'utf8mb4'
}

spider = MockSpider(db_config=db_config, image_dir="images")
spider.crawl(max_items=100)  # ç”Ÿæˆ100ä¸ªå•†å“
```

### ä½¿ç”¨çœŸå®çˆ¬è™«

```python
from spider import XianyuSpider

spider = XianyuSpider(db_config=db_config, image_dir="images")
spider.crawl(max_items=50)  # çˆ¬å–50ä¸ªå•†å“
```

## âš™ï¸ è‡ªå®šä¹‰çˆ¬è™«

### åˆ›å»ºæ–°çš„çˆ¬è™«ç±»

```python
from spider import ProductSpider

class MyCustomSpider(ProductSpider):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.base_url = "https://example.com"
    
    def crawl(self, max_items=50):
        # å®ç°ä½ çš„çˆ¬å–é€»è¾‘
        pass
    
    def parse_item(self, item_element):
        # è§£æå•†å“å…ƒç´ 
        return {
            'title': 'å•†å“æ ‡é¢˜',
            'price': 99.00,
            'category': 'æ•°ç ',
            'description': 'å•†å“æè¿°',
            'image_urls': ['å›¾ç‰‡URL'],
            'seller_id': 1
        }
```

## ğŸ”§ æ•°æ®å­—æ®µè¯´æ˜

çˆ¬è™«éœ€è¦è¿”å›ä»¥ä¸‹å­—æ®µï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | å¿…å¡« |
|------|------|------|------|
| title | str | å•†å“æ ‡é¢˜ | âœ… |
| price | float | å•†å“ä»·æ ¼ | âœ… |
| category | str | å•†å“åˆ†ç±» | âœ… |
| description | str | å•†å“æè¿° | â­• |
| original_price | float | åŸä»· | â­• |
| condition | str | æˆè‰²ï¼ˆå…¨æ–°/99æ–°ç­‰ï¼‰ | â­• |
| image_urls | list | å›¾ç‰‡URLåˆ—è¡¨ | â­• |
| seller_id | int | å–å®¶ID | â­• |

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. éµå®ˆç½‘ç«™è§„åˆ™
- æŸ¥çœ‹ç›®æ ‡ç½‘ç«™çš„ `robots.txt`
- éµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾
- ä¸è¦è¿‡åº¦é¢‘ç¹è¯·æ±‚

### 2. åçˆ¬è™«å¤„ç†
- å·²å†…ç½®éšæœºå»¶è¿Ÿ
- å·²è®¾ç½®User-Agent
- å»ºè®®ä½¿ç”¨ä»£ç†IPï¼ˆå¦‚éœ€è¦ï¼‰

### 3. æ•°æ®å»é‡
- çˆ¬è™«ä¼šè‡ªåŠ¨æ£€æŸ¥é‡å¤å•†å“ï¼ˆæ ¹æ®æ ‡é¢˜å’Œä»·æ ¼ï¼‰
- å·²å­˜åœ¨çš„å•†å“ä¼šè¢«è·³è¿‡

### 4. å›¾ç‰‡å­˜å‚¨
- å›¾ç‰‡ä¿å­˜åœ¨ `images/` ç›®å½•
- æ–‡ä»¶åæ ¼å¼ï¼š`goods_{å•†å“ID}_{ç´¢å¼•}.jpg`
- å›¾ç‰‡è·¯å¾„ä¼šä¿å­˜åˆ°æ•°æ®åº“çš„ `img_path` å­—æ®µ

## ğŸ› å¸¸è§é—®é¢˜

### Q: æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Ÿ
A: æ£€æŸ¥æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿MySQLæœåŠ¡æ­£åœ¨è¿è¡Œã€‚

### Q: å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Ÿ
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿å›¾ç‰‡URLå¯è®¿é—®ã€‚

### Q: çˆ¬å–é€Ÿåº¦å¤ªæ…¢ï¼Ÿ
A: å¯ä»¥è°ƒæ•´ `random_delay()` çš„å»¶è¿Ÿæ—¶é—´ï¼Œä½†è¦æ³¨æ„ä¸è¦è¢«å°ã€‚

### Q: å¦‚ä½•çˆ¬å–å…¶ä»–ç½‘ç«™ï¼Ÿ
A: ç»§æ‰¿ `ProductSpider` ç±»ï¼Œå®ç° `crawl()` å’Œ `parse_item()` æ–¹æ³•ã€‚

## ğŸ“š æ‰©å±•åŠŸèƒ½

### æ·»åŠ ä»£ç†æ”¯æŒ

```python
proxies = {
    'http': 'http://proxy.example.com:8080',
    'https': 'https://proxy.example.com:8080'
}
self.session.proxies.update(proxies)
```

### æ·»åŠ Cookieæ”¯æŒ

```python
self.session.cookies.update({
    'cookie_name': 'cookie_value'
})
```

### ä½¿ç”¨Seleniumï¼ˆå¤„ç†JavaScriptï¼‰

å¦‚æœéœ€è¦å¤„ç†JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼Œå¯ä»¥ä½¿ç”¨Seleniumï¼š

```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.get(url)
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
```

## ğŸ“„ è®¸å¯è¯

ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ã€‚

